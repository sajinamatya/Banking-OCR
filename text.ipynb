{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1049, 0.9324],\n",
      "        [0.7815, 0.1312]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "x = torch.rand(2, 2).cuda()\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsa = load_dataset(\"howard-hou/COCO-Text\",split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASTDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset containing the images and annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and annotations\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image']\n",
    "        ocr_info = sample['ocr_info']\n",
    "\n",
    "        # Convert image to RGB (in case it's in another format)\n",
    "        image = np.array(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert to BGR for OpenCV\n",
    "\n",
    "        # Resize image to fit EAST model input size (e.g., 512x512)\n",
    "        target_size = (512, 512)\n",
    "        image_resized = cv2.resize(image, target_size)\n",
    "\n",
    "        # Initialize a blank mask for text regions\n",
    "        height, width = target_size\n",
    "        text_mask = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "        # Initialize a list for bounding boxes\n",
    "        bboxes = []\n",
    "\n",
    "        for ann in ocr_info:\n",
    "            # Check if 'bbox' key exists before accessing it\n",
    "            if 'bounding_box' in ann:\n",
    "                bbox = ann['bounding_box']  # Format: [x, y, width, height]\n",
    "                x, y, w, h = bbox\n",
    "\n",
    "                # Scale bounding box to match resized image\n",
    "                x = int(x * (width / sample['image_width']))\n",
    "                y = int(y * (height / sample['image_height']))\n",
    "                w = int(w * (width / sample['image_width']))\n",
    "                h = int(h * (height / sample['image_height']))\n",
    "\n",
    "                # Draw a rectangle on the text mask\n",
    "                cv2.rectangle(text_mask, (x, y), (x + w, y + h), (1), -1)\n",
    "                bboxes.append([x, y, w, h])\n",
    "            # Handle cases where 'bbox' is missing (e.g., skip)\n",
    "            else:\n",
    "                #print(\"Warning: 'bbox' key not found in annotation. Skipping this annotation.\")\n",
    "                continue  # Skip this annotation if 'bbox' is missing\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image_tensor = transforms.ToTensor()(image_resized)\n",
    "\n",
    "        # Convert the bounding boxes to a tensor\n",
    "        # Check if bboxes is empty before creating a tensor to avoid errors\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32) if bboxes else torch.empty((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "\n",
    "        return image_tensor, text_mask, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EASTDataset(dsa)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EASTModel, self).__init__()\n",
    "\n",
    "\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])  # Remove avg pool and fc\n",
    "        \n",
    "        # Feature dimensions from ResNet50 will be 2048x7x7 for 224x224 input images\n",
    "        \n",
    "        # Upsampling layers to restore resolution (if needed)\n",
    "        # Add upsampling layers here if you need larger feature maps\n",
    "        \n",
    "        # Text score map output\n",
    "        self.text_score = nn.Conv2d(2048, 1, kernel_size=1)\n",
    "        \n",
    "        # Bounding box regression output (4 coordinates for each pixel)\n",
    "        self.bbox_reg = nn.Conv2d(2048, 4, kernel_size=1)\n",
    "        \n",
    "        # Angle regression output (for text rotation)\n",
    "        self.angle_map = nn.Conv2d(2048, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features from the backbone\n",
    "        features = self.backbone(x)  # This will be a 4D tensor [batch, 2048, H, W]\n",
    "        \n",
    "        # Predict text score map, bounding boxes, and angle\n",
    "        text_score = self.text_score(features)\n",
    "        bbox_reg = self.bbox_reg(features)\n",
    "        angle_map = self.angle_map(features)\n",
    "        \n",
    "        return text_score, bbox_reg, angle_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def east_loss(pred_text_score, pred_bbox, pred_angle, true_text_score, true_bbox, true_angle):\n",
    "    # Text score loss (binary cross-entropy)\n",
    "    text_loss = F.binary_cross_entropy_with_logits(pred_text_score, true_text_score)\n",
    "\n",
    "    # Bounding box regression loss (smooth L1 loss)\n",
    "    bbox_loss = F.smooth_l1_loss(pred_bbox, true_bbox)\n",
    "\n",
    "    # Angle regression loss (mean squared error)\n",
    "    angle_loss = F.mse_loss(pred_angle, true_angle)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = text_loss + bbox_loss + angle_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     10\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_masks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, type: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32me:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[13], line 42\u001b[0m, in \u001b[0;36mEASTDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     39\u001b[0m x, y, w, h \u001b[38;5;241m=\u001b[39m bbox\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Scale bounding box to match resized image\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_width\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     43\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(y \u001b[38;5;241m*\u001b[39m (height \u001b[38;5;241m/\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_height\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     44\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(w \u001b[38;5;241m*\u001b[39m (width \u001b[38;5;241m/\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_width\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = EASTModel()\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, text_masks, bboxes in train_loader:\n",
    "        images, text_masks, bboxes = images.cuda(), text_masks.cuda(), bboxes.cuda()\n",
    "        print(f\"x: {x}, type: {type(x)}\")\n",
    "        # Forward pass\n",
    "        pred_text_score, pred_bbox, pred_angle = model(images)\n",
    "        # Calculate the loss (using zero angle targets as a placeholder)\n",
    "        loss = east_loss(pred_text_score, pred_bbox, pred_angle, text_masks, bboxes, torch.zeros_like(bboxes))\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EASTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Get the CUDA device\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# ... your code to move your model and data to the GPU ...\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mEASTModel\u001b[49m()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# or model = EASTModel().cuda()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# input_tensor = input_tensor.to(device) # or input_tensor = input_tensor.cuda()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EASTModel' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    device = torch.device(\"cuda\")  # Get the CUDA device\n",
    "    # ... your code to move your model and data to the GPU ...\n",
    "    model = EASTModel().to(device) # or model = EASTModel().cuda()\n",
    "    # input_tensor = input_tensor.to(device) # or input_tensor = input_tensor.cuda()\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
