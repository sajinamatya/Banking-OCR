{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4754, 0.0326],\n",
      "        [0.6021, 0.3396]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "x = torch.rand(2, 2).cuda()\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsa = load_dataset(\"howard-hou/COCO-Text\",split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASTDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset containing the images and annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and annotations\n",
    "        sample = self.dataset[idx]\n",
    "        image = sample['image']\n",
    "        ocr_info = sample['ocr_info']\n",
    "\n",
    "        # Convert image to RGB (in case it's in another format)\n",
    "        image = np.array(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert to BGR for OpenCV\n",
    "\n",
    "        # Resize image to fit EAST model input size (e.g., 512x512)\n",
    "        target_size = (512, 512)\n",
    "        image_resized = cv2.resize(image, target_size)\n",
    "\n",
    "        # Initialize a blank mask for text regions\n",
    "        height, width = target_size\n",
    "        text_mask = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "        # Initialize a list for bounding boxes\n",
    "        bboxes = []\n",
    "\n",
    "        for ann in ocr_info:\n",
    "            # Check if 'bbox' key exists before accessing it\n",
    "            if 'bounding_box' in ann:\n",
    "                bbox = ann['bounding_box']  # Format: [x, y, width, height]\n",
    "                x, y, w, h = bbox\n",
    "\n",
    "                # Scale bounding box to match resized image\n",
    "                x = int(x * (width / sample['image_width']))\n",
    "                y = int(y * (height / sample['image_height']))\n",
    "                w = int(w * (width / sample['image_width']))\n",
    "                h = int(h * (height / sample['image_height']))\n",
    "\n",
    "                # Draw a rectangle on the text mask\n",
    "                cv2.rectangle(text_mask, (x, y), (x + w, y + h), (1), -1)\n",
    "                bboxes.append([x, y, w, h])\n",
    "            # Handle cases where 'bbox' is missing (e.g., skip)\n",
    "            else:\n",
    "                #print(\"Warning: 'bbox' key not found in annotation. Skipping this annotation.\")\n",
    "                continue  # Skip this annotation if 'bbox' is missing\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image_tensor = transforms.ToTensor()(image_resized)\n",
    "\n",
    "        # Convert the bounding boxes to a tensor\n",
    "        # Check if bboxes is empty before creating a tensor to avoid errors\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32) if bboxes else torch.empty((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "\n",
    "        return image_tensor, text_mask, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EASTDataset(dsa)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EASTModel, self).__init__()\n",
    "\n",
    "\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])  # Remove avg pool and fc\n",
    "        \n",
    "        # Feature dimensions from ResNet50 will be 2048x7x7 for 224x224 input images\n",
    "        \n",
    "        # Upsampling layers to restore resolution (if needed)\n",
    "        # Add upsampling layers here if you need larger feature maps\n",
    "        \n",
    "        # Text score map output\n",
    "        self.text_score = nn.Conv2d(2048, 1, kernel_size=1)\n",
    "        \n",
    "        # Bounding box regression output (4 coordinates for each pixel)\n",
    "        self.bbox_reg = nn.Conv2d(2048, 4, kernel_size=1)\n",
    "        \n",
    "        # Angle regression output (for text rotation)\n",
    "        self.angle_map = nn.Conv2d(2048, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features from the backbone\n",
    "        features = self.backbone(x)  # This will be a 4D tensor [batch, 2048, H, W]\n",
    "        \n",
    "        # Predict text score map, bounding boxes, and angle\n",
    "        text_score = self.text_score(features)\n",
    "        bbox_reg = self.bbox_reg(features)\n",
    "        angle_map = self.angle_map(features)\n",
    "        \n",
    "        return text_score, bbox_reg, angle_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def east_loss(pred_text_score, pred_bbox, pred_angle, true_text_score, true_bbox, true_angle):\n",
    "    # Text score loss (binary cross-entropy)\n",
    "    text_loss = F.binary_cross_entropy_with_logits(pred_text_score, true_text_score)\n",
    "\n",
    "    # Bounding box regression loss (smooth L1 loss)\n",
    "    bbox_loss = F.smooth_l1_loss(pred_bbox, true_bbox)\n",
    "\n",
    "    # Angle regression loss (mean squared error)\n",
    "    angle_loss = F.mse_loss(pred_angle, true_angle)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = text_loss + bbox_loss + angle_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._functorch.eager_transforms' has no attribute 'grad_and_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable()\n",
      "File \u001b[1;32me:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32me:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:25\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeprecated_func\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_symbolic_trace\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fx_tracing\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n",
      "File \u001b[1;32me:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:113\u001b[0m\n\u001b[0;32m    111\u001b[0m setup_docs(vmap, apis\u001b[38;5;241m.\u001b[39mvmap, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.vmap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    112\u001b[0m setup_docs(grad, apis\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m--> 113\u001b[0m \u001b[43msetup_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_and_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m setup_docs(vjp)\n\u001b[0;32m    115\u001b[0m setup_docs(jvp)\n",
      "File \u001b[1;32me:\\Sajilo E-bank\\.venv\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:44\u001b[0m, in \u001b[0;36msetup_docs\u001b[1;34m(functorch_api, torch_func_api, new_api_name)\u001b[0m\n\u001b[0;32m     42\u001b[0m api_name \u001b[38;5;241m=\u001b[39m functorch_api\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_func_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     torch_func_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# See https://docs.python.org/3/using/cmdline.html#cmdoption-OO\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_func_api\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch._functorch.eager_transforms' has no attribute 'grad_and_value'"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.debug = False\n",
    "torch._dynamo.disable()\n",
    "model = EASTModel()\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, text_masks, bboxes in train_loader:\n",
    "        images, text_masks, bboxes = images.cuda(), text_masks.cuda(), bboxes.cuda()\n",
    "        # Forward pass\n",
    "        pred_text_score, pred_bbox, pred_angle = model(images)\n",
    "        # Calculate the loss (using zero angle targets as a placeholder)\n",
    "        loss = east_loss(pred_text_score, pred_bbox, pred_angle, text_masks, bboxes, torch.zeros_like(bboxes))\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EASTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Get the CUDA device\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# ... your code to move your model and data to the GPU ...\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mEASTModel\u001b[49m()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# or model = EASTModel().cuda()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# input_tensor = input_tensor.to(device) # or input_tensor = input_tensor.cuda()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EASTModel' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    device = torch.device(\"cuda\")  # Get the CUDA device\n",
    "    # ... your code to move your model and data to the GPU ...\n",
    "    model = EASTModel().to(device) # or model = EASTModel().cuda()\n",
    "    # input_tensor = input_tensor.to(device) # or input_tensor = input_tensor.cuda()\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
