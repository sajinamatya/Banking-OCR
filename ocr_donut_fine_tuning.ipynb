{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-2S4JsdeEpD"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKbNhR8eeEpG"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "sajinamatya_ocr_donut_data_path = kagglehub.dataset_download('sajinamatya/ocr-donut-data')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-09T04:46:16.120911Z",
          "iopub.status.busy": "2025-04-09T04:46:16.120578Z",
          "iopub.status.idle": "2025-04-09T04:46:16.126371Z",
          "shell.execute_reply": "2025-04-09T04:46:16.125267Z",
          "shell.execute_reply.started": "2025-04-09T04:46:16.120881Z"
        },
        "id": "v62s984VeEpG",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:48:44.089388Z",
          "iopub.status.busy": "2025-04-09T04:48:44.089015Z",
          "iopub.status.idle": "2025-04-09T04:48:44.093842Z",
          "shell.execute_reply": "2025-04-09T04:48:44.09267Z",
          "shell.execute_reply.started": "2025-04-09T04:48:44.08936Z"
        },
        "id": "L4E9zodQeEpH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import gc\n",
        "from PIL import Image\n",
        "from datasets import Dataset\n",
        "from transformers import DonutProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:48:57.067626Z",
          "iopub.status.busy": "2025-04-09T04:48:57.067335Z",
          "iopub.status.idle": "2025-04-09T04:48:57.149492Z",
          "shell.execute_reply": "2025-04-09T04:48:57.148635Z",
          "shell.execute_reply.started": "2025-04-09T04:48:57.067604Z"
        },
        "id": "eruyH_iqeEpH",
        "outputId": "a65b1539-14f8-4beb-fa8f-67a099652f9d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "GPU: Tesla P100-PCIE-16GB\n",
            "Available memory: 17.06 GB\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:59:59.989565Z",
          "iopub.status.busy": "2025-04-09T04:59:59.989219Z",
          "iopub.status.idle": "2025-04-09T04:59:59.99329Z",
          "shell.execute_reply": "2025-04-09T04:59:59.992344Z",
          "shell.execute_reply.started": "2025-04-09T04:59:59.98954Z"
        },
        "id": "7DPUglXleEpI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "train_metadata_path = \"/kaggle/input/ocr-donut-data/data/train/metadata.jsonl\"\n",
        "val_metadata_path = \"/kaggle/input/ocr-donut-data/data/val/metadata.jsonl\"\n",
        "test_metadata_path = \"/kaggle/input/ocr-donut-data/data/test/metadata.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:49:19.346342Z",
          "iopub.status.busy": "2025-04-09T04:49:19.346013Z",
          "iopub.status.idle": "2025-04-09T04:49:19.350603Z",
          "shell.execute_reply": "2025-04-09T04:49:19.349767Z",
          "shell.execute_reply.started": "2025-04-09T04:49:19.346313Z"
        },
        "id": "Ik18BfnheEpI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def resize_image(image, max_size=(384, 384)):\n",
        "    \"\"\"Resize image while maintaining aspect ratio\"\"\"\n",
        "    width, height = image.size\n",
        "    if width > max_size[0] or height > max_size[1]:\n",
        "        image.thumbnail(max_size, Image.LANCZOS)\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1321b76e996f4726af5d962d1f749074",
            "d6f79e6c7205476bacff2a7b570b15c5",
            "8ca5060277e548a09134c99295360c8e",
            "d5443c6c2be04ed89277e3eb981bc391",
            "8f54894710ad4feb83deeb347466e157",
            "7a59238b73f644d580898a1ad84c2fbb"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-09T04:50:18.178211Z",
          "iopub.status.busy": "2025-04-09T04:50:18.177917Z",
          "iopub.status.idle": "2025-04-09T04:50:23.341858Z",
          "shell.execute_reply": "2025-04-09T04:50:23.341157Z",
          "shell.execute_reply.started": "2025-04-09T04:50:18.17819Z"
        },
        "id": "JBinScXEeEpJ",
        "outputId": "85050951-43f7-4631-c8c3-479b1a6b754c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_name = \"naver-clova-ix/donut-base\"\n",
        "processor = DonutProcessor.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a861b974a8f2431287ea8bb66cb1b1e2",
            "73944c548d7f4cafb84e45f4daef82de"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-09T04:50:39.704579Z",
          "iopub.status.busy": "2025-04-09T04:50:39.7042Z",
          "iopub.status.idle": "2025-04-09T04:50:46.909484Z",
          "shell.execute_reply": "2025-04-09T04:50:46.90864Z",
          "shell.execute_reply.started": "2025-04-09T04:50:39.704549Z"
        },
        "id": "fZOjRAn8eEpJ",
        "outputId": "3037c8e1-07f3-4ab5-d42f-f9cedb41446b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = VisionEncoderDecoderModel.from_pretrained(\n",
        "    model_name,\n",
        "\n",
        "    low_cpu_mem_usage=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:50:59.33698Z",
          "iopub.status.busy": "2025-04-09T04:50:59.336632Z",
          "iopub.status.idle": "2025-04-09T04:50:59.340942Z",
          "shell.execute_reply": "2025-04-09T04:50:59.340082Z",
          "shell.execute_reply.started": "2025-04-09T04:50:59.336948Z"
        },
        "id": "3y9qvKVbeEpK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.config.decoder_start_token_id = processor.tokenizer.bos_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O229x1mNeEpK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:51:15.00141Z",
          "iopub.status.busy": "2025-04-09T04:51:15.000951Z",
          "iopub.status.idle": "2025-04-09T04:51:15.008791Z",
          "shell.execute_reply": "2025-04-09T04:51:15.007981Z",
          "shell.execute_reply.started": "2025-04-09T04:51:15.001373Z"
        },
        "id": "MKrX7Dg4eEpK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_data_from_metadata(metadata_path, max_samples=None):\n",
        "    # Get base directory from metadata path\n",
        "    base_dir = os.path.dirname(metadata_path)\n",
        "    metadata = []\n",
        "    processed = 0\n",
        "\n",
        "    print(f\"Reading metadata from {metadata_path}...\")\n",
        "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            if max_samples and processed >= max_samples:\n",
        "                print(f\"Reached max samples limit ({max_samples})\")\n",
        "                break\n",
        "\n",
        "            if not line.strip():\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                item = json.loads(line)\n",
        "                # Parse the ground_truth string into a JSON object\n",
        "                gt = json.loads(item[\"ground_truth\"])\n",
        "                parsed = gt[\"gt_parse\"]\n",
        "\n",
        "                # Format the text in a structured way\n",
        "                text = f\"\"\"citizenship_no: {parsed['citizenship_certificate_no']}\n",
        "name: {parsed['full_name']}\n",
        "sex: {parsed['sex']}\n",
        "dob: {parsed['date_of_birth']['year']}-{parsed['date_of_birth']['month']}-{parsed['date_of_birth']['day']}\n",
        "birth_place: {parsed['birth_place']['district']}, {parsed['birth_place']['vdc']}-{parsed['birth_place']['ward']}\n",
        "address: {parsed['permament_address']['district']}, {parsed['permament_address']['vdc']}-{parsed['permament_address']['ward']}\"\"\"\n",
        "\n",
        "                # Build complete image path\n",
        "                image_path = os.path.join(base_dir, item[\"file_name\"])\n",
        "\n",
        "                # Verify image exists\n",
        "                if os.path.exists(image_path):\n",
        "                    metadata.append({\n",
        "                        \"image_path\": image_path,\n",
        "                        \"text\": text\n",
        "                    })\n",
        "                    processed += 1\n",
        "                    if processed % 100 == 0:\n",
        "                        print(f\"Processed {processed} valid entries\")\n",
        "                else:\n",
        "                    print(f\"Warning: Image not found at {image_path}\")\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing line {line_num} in {metadata_path}: {e}\")\n",
        "                continue\n",
        "            except KeyError as e:\n",
        "                print(f\"Missing key in data at line {line_num}: {e}\")\n",
        "                continue\n",
        "\n",
        "    if not metadata:\n",
        "        raise ValueError(f\"No valid data loaded from {metadata_path}\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(metadata)} entries from {metadata_path}\")\n",
        "    return metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:51:24.705529Z",
          "iopub.status.busy": "2025-04-09T04:51:24.705142Z",
          "iopub.status.idle": "2025-04-09T04:51:24.711214Z",
          "shell.execute_reply": "2025-04-09T04:51:24.710339Z",
          "shell.execute_reply.started": "2025-04-09T04:51:24.705485Z"
        },
        "id": "IPhMdH1weEpK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(data, max_samples=None):\n",
        "    images = []\n",
        "    ground_truths = []\n",
        "\n",
        "    if max_samples and len(data) > max_samples:\n",
        "        print(f\"Limiting dataset to {max_samples} samples (from {len(data)} available)\")\n",
        "        data = data[:max_samples]\n",
        "\n",
        "    for i, item in enumerate(data):\n",
        "        try:\n",
        "            image = Image.open(item[\"image_path\"]).convert(\"RGB\")\n",
        "            # Resize images to save memory\n",
        "            image = resize_image(image)\n",
        "            ground_truth = item[\"text\"]\n",
        "            images.append(image)\n",
        "            ground_truths.append(ground_truth)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f\"Prepared {i+1}/{len(data)} images\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {item['image_path']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"No valid images processed\")\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"image\": images,\n",
        "        \"ground_truth\": ground_truths\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:51:31.881911Z",
          "iopub.status.busy": "2025-04-09T04:51:31.881632Z",
          "iopub.status.idle": "2025-04-09T04:51:31.887196Z",
          "shell.execute_reply": "2025-04-09T04:51:31.886298Z",
          "shell.execute_reply.started": "2025-04-09T04:51:31.881891Z"
        },
        "id": "TRiEE7l1eEpL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    try:\n",
        "        # Process images\n",
        "        pixel_values = processor(\n",
        "            examples[\"image\"],\n",
        "            padding=\"max_length\",\n",
        "            max_length=256, # Reduced sequence length\n",
        "            return_tensors=\"pt\",\n",
        "        ).pixel_values\n",
        "\n",
        "        # Process text\n",
        "        task_prompt = \"<s_ocr>\"\n",
        "        decoder_input_ids = processor.tokenizer(\n",
        "            [task_prompt + gt for gt in examples[\"ground_truth\"]],\n",
        "            padding=\"max_length\",\n",
        "            max_length=256, # Reduced sequence length\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids\n",
        "\n",
        "        labels = decoder_input_ids.clone()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in preprocessing: {e}\")\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "861e40b79d7c4e039c0ac98357cf834b",
            "2d920f62fbe04a089ea3dc95014ec26c"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-04-09T05:00:08.960717Z",
          "iopub.status.busy": "2025-04-09T05:00:08.960377Z",
          "iopub.status.idle": "2025-04-09T05:01:29.084709Z",
          "shell.execute_reply": "2025-04-09T05:01:29.084032Z",
          "shell.execute_reply.started": "2025-04-09T05:00:08.960687Z"
        },
        "id": "eSi7ersXeEpL",
        "outputId": "372b1eb8-1f78-4956-c384-103e7dd8d80c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load and prepare train data with memory limits\n",
        "print(\"Loading training data...\")\n",
        "train_max_samples = 200  # Adjust based on your dataset size and memory constraints\n",
        "train_data = load_data_from_metadata(train_metadata_path, max_samples=train_max_samples)\n",
        "\n",
        "\n",
        "print(\"Preparing training dataset...\")\n",
        "train_dataset = prepare_dataset(train_data)\n",
        "del train_data\n",
        "\n",
        "\n",
        "print(\"Processing training dataset...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=2,  # Very small batch for preprocessing\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "\n",
        "# Load and prepare validation data with memory limits\n",
        "print(\"Loading validation data...\")\n",
        "val_max_samples = 200  # Adjust based on your dataset size and memory constraints\n",
        "val_data = load_data_from_metadata(val_metadata_path, max_samples=val_max_samples)\n",
        "\n",
        "\n",
        "print(\"Preparing validation dataset...\")\n",
        "val_dataset = prepare_dataset(val_data)\n",
        "del val_data\n",
        "\n",
        "print(\"Processing validation dataset...\")\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=2,  # Very small batch for preprocessing\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T05:02:21.655268Z",
          "iopub.status.busy": "2025-04-09T05:02:21.654944Z",
          "iopub.status.idle": "2025-04-09T05:56:21.457804Z",
          "shell.execute_reply": "2025-04-09T05:56:21.457091Z",
          "shell.execute_reply.started": "2025-04-09T05:02:21.655223Z"
        },
        "id": "3T7N5jspeEpM",
        "outputId": "aa715e15-0825-4d82-b87a-1bdeaf10e4b3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/kaggle/working/model\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=1, # Reduced batch size\n",
        "    per_device_eval_batch_size=1, # Reduced batch size\n",
        "    gradient_accumulation_steps=4, # Reduced gradient accumulation steps\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True, # Enable mixed precision training\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    dataloader_num_workers=0,\n",
        "    gradient_checkpointing=True,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=0.5,  # Add gradient clipping\n",
        "    report_to=\"none\"  # Disable reporting to save memory\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "trainer.save_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMzsGMraeEpN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/kaggle/working/pretrain\")\n",
        "processor.save_pretrained(\"/kaggle/working/pretrain\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ocr_donut",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "datasetId": 7090723,
          "sourceId": 11335323,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
